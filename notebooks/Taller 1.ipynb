{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Primer Taller Inteligencia Artificial.**\\\n",
    "**1.**\\\n",
    "$\\hspace{0.1cm}$**i.** Se tienen los siguientes ejemplos de entrenamiento para un problema de regresión lineal:\n",
    "$$\n",
    "<\\left(\\begin{array}{l}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right), 0>,<\\left(\\begin{array}{c}\n",
    "-1 \\\\\n",
    "0\n",
    "\\end{array}\\right), 1 / 2>,<\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right), 1 / 2>,<\\left(\\begin{array}{l}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\\right), 1>,<\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right), 3 / 2>\n",
    "$$\n",
    "Si el vector de pesos es $\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right)$ aplique el algoritmo de **Descenso Gradiente Estocástico** para modificar el vector de pesos. Calcule la reducción en la funcion LossTrain al hacer un ciclo.\\\n",
    "\\\n",
    "**Solución**\n",
    "\\\n",
    "\\\n",
    "**Algortimo de Descenso gradiente estocástico para problemas de regresión lineal**\\\n",
    "Para cada $(x, y) \\in \\mathcal{D}_{\\text {train }}:$\\\n",
    "$\\hspace{0.7cm}\\mathbf{w} \\leftarrow \\mathbf{w}-\\underbrace{\\eta}_{t p} \\underbrace{\\nabla_{\\mathbf{w}} \n",
    "TrainLoss(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})}_{\\text {gradiente }}$\n",
    "\\\n",
    "\\\n",
    "Podemos trabajar con \\\n",
    "\\\n",
    "$n=0.5=\\frac{1}{2}$\\\n",
    "\\\n",
    "Nuestra función gradiente es:\\\n",
    "\\\n",
    "$\\nabla_{\\mathrm{w}} TrainLoss(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})=2((\\mathrm{w} \\cdot \\phi(\\mathrm{x}))-\\mathrm{y}) \\phi(\\mathrm{x})$\\\n",
    "\\\n",
    "Nuestra función de perdida es:\\\n",
    "\\\n",
    "$TrainLoss(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})=\\frac{1}{2}(X \\mathbf{w}-\\mathbf{y})^{t}(X \\mathbf{w}-\\mathbf{y})$\\\n",
    "\\\n",
    "Donde:\\\n",
    "$\n",
    "X \\mathbf{w}-\\mathbf{y}=\\left(\\begin{array}{c}\n",
    "\\left(\\mathbf{x}^{1}\\right)^{t} \\\\\n",
    "\\left(\\mathrm{x}^{2}\\right)^{t} \\\\\n",
    "\\vdots \\\\\n",
    "\\left(\\mathbf{x}^{\\mathrm{m}}\\right)^{t}\n",
    "\\end{array}\\right) \\mathbf{w}-\\left(\\begin{array}{c}\n",
    "y^{1} \\\\\n",
    "y^{2} \\\\\n",
    "\\vdots \\\\\n",
    "y^{m}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "\\left(\\mathbf{x}^{1}\\right)^{t} \\mathbf{w}-y^{1} \\\\\n",
    "\\left.\\mathbf{x}^{2}\\right)^{t} \\mathbf{w}-y^{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\left.\\mathbf{x}^{\\mathbf{m}}\\right)^{t} \\mathbf{w}-y^{m}\n",
    "\\end{array}\\right)\n",
    "$\\\n",
    "\\\n",
    "**Hagamos la primera iteración del algoritmo manualmente:**\\\n",
    "\\\n",
    "Tenemos que $\\mathrm{w}=\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right),\\hspace{0.2cm}\\mathrm{x}_{\\mathrm{1}}=\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right),\\hspace{0.2cm}\\mathrm{y}=0$\\\n",
    "\\\n",
    "No olvidemos que en cada iteración $\\hspace{0.2cm}\\phi(\\mathrm{x})=\\mathrm{x}_{\\mathrm{n}}$\\\n",
    "\\\n",
    "Procedemos a **hacer los cálculos** y obtenemos que:\\\n",
    "\\\n",
    "$\\nabla_{\\mathrm{w}} TrainLoss(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})=2((\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right))-0) \\cdot \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)=\\left(\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right)$\\\n",
    "\\\n",
    "En la primera iteración actualizamos $\\mathrm{w}$ de la siguiente forma:\\\n",
    "\\\n",
    "$\\mathrm{w}=\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right)-\\frac{1}{2}\\cdot \\left(\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right)=\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right)$\\\n",
    "\\\n",
    "Como podemos ver en la primera iteración $\\mathrm{w}$ no sufrio ningún cambio.\\\n",
    "\\\n",
    "Ahora veamos la perdida de entrenamiento:\\\n",
    "\\\n",
    "$X \\mathbf{w}-\\mathbf{y}=\\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0 \\\\\n",
    "1 & -1 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{array}\\right)\\left(\\begin{array}{l}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right)-\\left(\\begin{array}{c}\n",
    "0 \\\\\n",
    "\\frac{1}{2} \\\\\n",
    "\\frac{1}{2} \\\\\n",
    "1 \\\\\n",
    "\\frac{3}{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "0 \\\\\n",
    "-\\frac{3}{2} \\\\\n",
    "\\frac{1}{2} \\\\\n",
    "0 \\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)$\\\n",
    "\\\n",
    "$(X \\mathbf{w}-\\mathbf{y})^{t}=\\left(\\begin{array}{lllll}\n",
    "0 & -\\frac{3}{2} & \\frac{1}{2} & 0 & \\frac{1}{2}\n",
    "\\end{array}\\right)$\\\n",
    "\\\n",
    "$TrainLoss(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})= \\frac{1}{2}\\left(\\begin{array}{lllll}\n",
    "0 & -\\frac{3}{2} & \\frac{1}{2} & 0 & \\frac{1}{2}\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{c}\n",
    "0 \\\\\n",
    "-\\frac{3}{2} \\\\\n",
    "\\frac{1}{2} \\\\\n",
    "0 \\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)$\\\n",
    "\\\n",
    "$TrainLoss(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})= \\frac{11}{8}=1.375$\\\n",
    "\\\n",
    "Esta solo fue una iteración del algoritmo, ahora vamos a ver todas las iteraciones del algoritmo.\\\n",
    "\\\n",
    "**Corramos todo el algoritmo en Python**"
   ],
   "attachments": {},
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "MD",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# We define our Train Loss fuction (1/2)*np.transpose(X*w-y)*(X*w-y)\n",
    "def train_loss(d_train, w):\n",
    "    # We create the matrix X and the matriz y\n",
    "    X = []\n",
    "    y = []\n",
    "    for data in d_train:\n",
    "        X.append([1] + data[0]), y.append(data[1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    w = np.array(w)\n",
    "    return (1/2)*np.dot(np.transpose((np.dot(X, w) - y)), (np.dot(X, w) - y))\n",
    "\n",
    "\n",
    "# We define our Train Loss Gradient fuction (ϕ(x) = x)\n",
    "def train_loss_gradient(x, y, w):\n",
    "    return 2 * np.dot((np.dot(w, x) - y), x)\n",
    "\n",
    "\n",
    "# We define our Gradient descent algorithm\n",
    "def stochastic_gradient_descent(d_train, w, n, iterations):\n",
    "    print(\"Running stochastic gradient descent algorithm\")\n",
    "    iteration = 1\n",
    "    j = 0\n",
    "    while iteration <= iterations:\n",
    "        y = d_train[j][1]\n",
    "        # We add 1 as the first component in x\n",
    "        x = np.array([1] + d_train[j][0])\n",
    "        # We calculate the Train Loss Gradient\n",
    "        train_loss_gradient_ = train_loss_gradient(x, y, w)\n",
    "        # We update the vector w\n",
    "        w = w - n * train_loss_gradient_\n",
    "        # We calculate the Train Loss\n",
    "        train_loss_ = train_loss(d_train, w)\n",
    "        # We print the data\n",
    "        print(f'In the iteration {iteration}, we get '\n",
    "              f'w = {w} and TrainLoss(x, y, w) = {train_loss_}')\n",
    "        iteration += 1\n",
    "        if j == len(d_train) - 1:\n",
    "            j = 0\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    print('Algoritm finished')\n",
    "\n",
    "\n",
    "# We define our DTrain, our weight vector, our n and our iteration number\n",
    "d_train = [[[0, 0], 0], [[-1, 0], 1 / 2], [[1, 0], 1 / 2], [[0, 1], 1], [[1, 1], 3 / 2]]\n",
    "w = np.array([0, 1, 1])\n",
    "n = 0.5\n",
    "iterations = 5\n",
    "\n",
    "# We run our stochastic gradient descent algorithm\n",
    "stochastic_gradient_descent(d_train, w, n, iterations)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running stochastic gradient descent algorithm\n",
      "In the iteration 1, we get w = [0. 1. 1.] and TrainLoss(x, y, w) = 1.375\n",
      "In the iteration 2, we get w = [ 1.5 -0.5  1. ] and TrainLoss(x, y, w) = 3.625\n",
      "In the iteration 3, we get w = [ 1. -1.  1.] and TrainLoss(x, y, w) = 2.375\n",
      "In the iteration 4, we get w = [ 0. -1.  0.] and TrainLoss(x, y, w) = 4.875\n",
      "In the iteration 5, we get w = [2.5 1.5 2.5] and TrainLoss(x, y, w) = 29.875\n",
      "Algoritm finished\n"
     ]
    }
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "CODE",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\hspace{0.1cm}$**ii.**   Considere los mismos ejemplos de entrenamiento pero para un problema de\n",
    "clasificación:\n",
    "$$\n",
    "<\\left(\\begin{array}{l}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right), 1>,<\\left(\\begin{array}{c}\n",
    "-1 \\\\\n",
    "0\n",
    "\\end{array}\\right), -1>,<\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right), 1>,<\\left(\\begin{array}{l}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\\right), -1>,<\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right), 1>\n",
    "$$\n",
    "Para el mismo vector de pesos $\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right)$ aplique el algoritmo de **Descenso Gradiente Estocástico** para modificarlo.\\\n",
    "\\\n",
    "**Solución**\n",
    "\\\n",
    "\\\n",
    "**Algortimo de Descenso gradiente estocástico para problemas de clasificación**\\\n",
    "Para cada $(x, y) \\in \\mathcal{D}_{\\text {train }}:$\\\n",
    "$\\hspace{0.7cm}\\mathbf{w} \\leftarrow \\mathbf{w}-\\underbrace{\\eta}_{t p} \\underbrace{\\nabla_{\\mathbf{w}} {TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})}_{\\text {gradiente }}$\\\n",
    "\\\n",
    "Podemos trabajar con \\\n",
    "\\\n",
    "$n=0.5=\\frac{1}{2}$\\\n",
    "\\\n",
    "Nuestra función gradiente es:\\\n",
    "\\\n",
    "$\\nabla_{\\mathbf{w}} {TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})=-1[\\mathbf{w} \\cdot \\phi(\\mathbf{x}))\\mathbf{y} < 1] \\cdot\\phi(\\mathbf{x}) \\cdot \\mathbf{y}$\\\n",
    "\\\n",
    "Nuestra función de perdida es:\\\n",
    "\\\n",
    "${TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})= \\sum_{(\\mathbf{x}, y) \\in D_{\\text {train }}} (\\max \\{1-(\\mathbf{w} \\cdot \\phi(\\mathbf{x}))\\mathbf{y}, 0\\})$\\\n",
    "\\\n",
    "**Hagamos la primera iteración del algoritmo manualmente:**\\\n",
    "\\\n",
    "Tenemos que $\\mathrm{w}=\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right),\\hspace{0.2cm}\\mathrm{x}_{\\mathrm{1}}=\\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right),\\hspace{0.2cm}\\mathrm{y}=1$\\\n",
    "\\\n",
    "No olvidemos que en cada iteración $\\hspace{0.2cm}\\phi(\\mathrm{x})=\\mathrm{x}_{\\mathrm{n}}$\\\n",
    "\\\n",
    "Procedemos a **hacer los cálculos** y obtenemos que:\\\n",
    "\\\n",
    "$\\nabla_{\\mathrm{w}} {TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})=-1[\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right) \\cdot 1 < 1] \\cdot \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right) \\cdot 1 = \\left(\\begin{array}{l}-1 \\\\ \\hspace{0.2cm}0 \\\\ \\hspace{0.2cm}0\\end{array}\\right)$\\\n",
    "\\\n",
    "En la primera iteración actualizamos $\\mathrm{w}$ de la siguiente forma:\\\n",
    "\\\n",
    "$\\mathrm{w}=\\left(\\begin{array}{l}0 \\\\ 1 \\\\ 1\\end{array}\\right)-\\frac{1}{2}\\cdot \\left(\\begin{array}{l}-1 \\\\ \\hspace{0.2cm}0 \\\\ \\hspace{0.2cm}0\\end{array}\\right)=\\left(\\begin{array}{l}\\frac{1}{2} \\\\ \\hspace{0.1cm} 1 \\\\ \\hspace{0.1cm} 1\\end{array}\\right)$\\\n",
    "\\\n",
    "Ahora veamos la perdida de entrenamiento:\\\n",
    "\\\n",
    "${TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})=\\max \\{1-(\\left(\\begin{array}{l}\\frac{1}{2} \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 0\\end{array}\\right)) \\cdot 1, \\hspace{0.2cm} 0\\} + \\max \\{1-(\\left(\\begin{array}{l}\\frac{1}{2} \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ -1 \\\\ 0\\end{array}\\right)) \\cdot -1, \\hspace{0.2cm} 0\\} + \\max \\{1-(\\left(\\begin{array}{l}\\frac{1}{2} \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ 1 \\\\ 0\\end{array}\\right)) \\cdot 1, \\hspace{0.2cm} 0\\} + \\max \\{1-(\\left(\\begin{array}{l}\\frac{1}{2} \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ 0 \\\\ 1\\end{array}\\right)) \\cdot -1, \\hspace{0.2cm} 0\\} + \\max \\{1-(\\left(\\begin{array}{l}\\frac{1}{2} \\\\ 1 \\\\ 1\\end{array}\\right) \\cdot \\left(\\begin{array}{l}1 \\\\ 1 \\\\ 1\\end{array}\\right)) \\cdot 1, \\hspace{0.2cm} 0\\}$\\\n",
    "\\\n",
    "${TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})= \\max \\{ \\frac{1}{2}, \\hspace{0.1cm} 0\\} + \\max \\{ \\frac{1}{2}, \\hspace{0.1cm} 0\\} + \\max \\{ -\\frac{1}{2}, \\hspace{0.1cm} 0\\} + \\max \\{ \\frac{5}{2}, \\hspace{0.1cm} 0\\} + \\max \\{ -\\frac{3}{2}, \\hspace{0.1cm} 0\\}$\\\n",
    "\\\n",
    "${TrainLoss}_{Hinge}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})= \\frac{1}{2}+\\frac{1}{2}+0+\\frac{5}{2}+0=\\frac{7}{2}=3.5$\\\n",
    "\\\n",
    "Esta solo fue una iteración del algoritmo, ahora vamos a ver todas las iteraciones del algoritmo.\\\n",
    "\\\n",
    "**Corramos todo el algoritmo en Python**"
   ],
   "attachments": {},
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "MD",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# We define our Hinge Train Loss fuction\n",
    "def hinge_train_loss(d_train, w):\n",
    "    summation = 0\n",
    "    for data in d_train:\n",
    "        x = np.array([1] + data[0])\n",
    "        y = data[1]\n",
    "        summation += max(1-np.dot(w, x)*y, 0)\n",
    "    return summation\n",
    "\n",
    "\n",
    "# We define our Hinge Train Loss Gradient fuction (ϕ(x) = x)\n",
    "def hinge_train_loss_gradient(x, y, w):\n",
    "    # We add 1 as the first component in x\n",
    "    x = np.array([1] + x)\n",
    "    return -1*(np.dot(w, x)*y < 1)*x*y\n",
    "\n",
    "\n",
    "# We define our Gradient descent algorithm\n",
    "def stochastic_gradient_descent(d_train, w, n, iterations):\n",
    "    print(\"Running stochastic gradient descent algorithm\")\n",
    "    iteration = 1\n",
    "    j = 0\n",
    "    while iteration <= iterations:\n",
    "        y = d_train[j][1]\n",
    "        # We add 1 as the first component in x\n",
    "        x = d_train[j][0]\n",
    "        # We calculate the Hinge Train Loss Gradient\n",
    "        hinge_train_loss_gradient_ = hinge_train_loss_gradient(x, y, w)\n",
    "        # We update the vector w\n",
    "        w = w - n * hinge_train_loss_gradient_\n",
    "        # We calculate the Hinge Train Loss\n",
    "        hinge_train_loss_ = hinge_train_loss(d_train, w)\n",
    "        # We print the data\n",
    "        print(f'In the iteration {iteration}, we get '\n",
    "              f'w = {w} and TrainLossHinge(x, y, w) = {hinge_train_loss_}')\n",
    "        iteration += 1\n",
    "        if j == len(d_train) - 1:\n",
    "            j = 0\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    print('Algoritm finished')\n",
    "\n",
    "\n",
    "# We define our DTrain, our weight vector and our n\n",
    "d_train = [[[0, 0], 1], [[-1, 0], -1], [[1, 0], 1], [[0, 1], -1], [[1, 1], 1]]\n",
    "w = np.array([0, 1, 1])\n",
    "n = 0.5\n",
    "iterations = 41\n",
    "\n",
    "# We run our stochastic gradient descent algorithm\n",
    "stochastic_gradient_descent(d_train, w, n, iterations)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Running stochastic gradient descent algorithm\n",
      "In the iteration 1, we get w = [0.5 1.  1. ] and TrainLossHinge(x, y, w) = 3.5\n",
      "In the iteration 2, we get w = [0.  1.5 1. ] and TrainLossHinge(x, y, w) = 3.0\n",
      "In the iteration 3, we get w = [0.  1.5 1. ] and TrainLossHinge(x, y, w) = 3.0\n",
      "In the iteration 4, we get w = [-0.5  1.5  0.5] and TrainLossHinge(x, y, w) = 2.5\n",
      "In the iteration 5, we get w = [-0.5  1.5  0.5] and TrainLossHinge(x, y, w) = 2.5\n",
      "In the iteration 6, we get w = [0.  1.5 0.5] and TrainLossHinge(x, y, w) = 2.5\n",
      "In the iteration 7, we get w = [0.  1.5 0.5] and TrainLossHinge(x, y, w) = 2.5\n",
      "In the iteration 8, we get w = [0.  1.5 0.5] and TrainLossHinge(x, y, w) = 2.5\n",
      "In the iteration 9, we get w = [-0.5  1.5  0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 10, we get w = [-0.5  1.5  0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 11, we get w = [0.  1.5 0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 12, we get w = [0.  1.5 0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 13, we get w = [0.  1.5 0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 14, we get w = [-0.5  1.5 -0.5] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 15, we get w = [0. 2. 0.] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 16, we get w = [0.5 2.  0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 17, we get w = [0.5 2.  0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 18, we get w = [0.5 2.  0. ] and TrainLossHinge(x, y, w) = 2.0\n",
      "In the iteration 19, we get w = [ 0.   2.  -0.5] and TrainLossHinge(x, y, w) = 1.5\n",
      "In the iteration 20, we get w = [ 0.   2.  -0.5] and TrainLossHinge(x, y, w) = 1.5\n",
      "In the iteration 21, we get w = [ 0.5  2.  -0.5] and TrainLossHinge(x, y, w) = 1.5\n",
      "In the iteration 22, we get w = [ 0.5  2.  -0.5] and TrainLossHinge(x, y, w) = 1.5\n",
      "In the iteration 23, we get w = [ 0.5  2.  -0.5] and TrainLossHinge(x, y, w) = 1.5\n",
      "In the iteration 24, we get w = [ 0.  2. -1.] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 25, we get w = [ 0.  2. -1.] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 26, we get w = [ 0.5  2.  -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 27, we get w = [ 0.5  2.  -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 28, we get w = [ 0.5  2.  -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 29, we get w = [ 0.   2.  -1.5] and TrainLossHinge(x, y, w) = 1.5\n",
      "In the iteration 30, we get w = [ 0.5  2.5 -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 31, we get w = [ 1.   2.5 -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 32, we get w = [ 1.   2.5 -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 33, we get w = [ 1.   2.5 -1. ] and TrainLossHinge(x, y, w) = 1.0\n",
      "In the iteration 34, we get w = [ 0.5  2.5 -1.5] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 35, we get w = [ 0.5  2.5 -1.5] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 36, we get w = [ 1.   2.5 -1.5] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 37, we get w = [ 1.   2.5 -1.5] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 38, we get w = [ 1.   2.5 -1.5] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 39, we get w = [ 0.5  2.5 -2. ] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 40, we get w = [ 0.5  2.5 -2. ] and TrainLossHinge(x, y, w) = 0.5\n",
      "In the iteration 41, we get w = [ 1.   2.5 -2. ] and TrainLossHinge(x, y, w) = 0.0\n",
      "Algoritm finished\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "CODE",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como vimos anteriormente, ensayando a pura prueba y error me di cuenta que con 41 iteraciones el algoritmo converge en una respuesta correcta.\\\n",
    "\\\n",
    "Hallemos con el vector **w**, la recta que clasifica de manera correcta los puntos:\\\n",
    "\\\n",
    "El vector que hallamos es $\\mathbf{w}=\\left(\\begin{array}{c}1 \\\\ \\frac{5}{2} \\\\ -2\\end{array}\\right)$, Para hallar la ecuación de la recta usamos la siguiente ecuación:\\\n",
    "\\\n",
    "$\\mathbf{w} \\cdot \\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "x \\\\\n",
    "y\n",
    "\\end{array}\\right)=0$\\\n",
    "\\\n",
    "Calculemos:\\\n",
    "\\\n",
    "$\n",
    "\\begin{aligned}\n",
    "&\\left(\\begin{array}{c}\n",
    "1 \\\\\n",
    "\\frac{5}{2} \\\\\n",
    "-2\n",
    "\\end{array}\\right) \\cdot\\left(\\begin{array}{l}\n",
    "1 \\\\\n",
    "x \\\\\n",
    "y\n",
    "\\end{array}\\right)=0 \\\\\n",
    "\\\\\n",
    "&1+\\frac{5}{2} x-2 y=0\n",
    "\\end{aligned}\n",
    "$\\\n",
    "\\\n",
    "Despejando:\\\n",
    "\\\n",
    "$y=\\frac{5}{4} x+\\frac{1}{2}$\\\n",
    "\\\n",
    "Veámoslo gráficamente\\\n",
    "\\\n",
    "<img src=\"https://storage.googleapis.com/bucket-adomi-dev/images/Gr%C3%A1fica.png\"/>\\\n",
    "\\\n",
    "Los puntos rojos son los de clase **-1** y los puntos azules son los de clase **1**, como podemos ver, la gráfica hace la clasificación perfectamente."
   ],
   "attachments": {},
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "MD",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.** En tanto que objeto y sistema, el cerebro humano es especial: su conectividad,\n",
    "su dinámica, su forma de funcionamiento, su relación con el cuerpo y con el\n",
    "mundo, no se parece a nada que la ciencia conozca. Su carácter único hace que\n",
    "el ofrecer una imagen del cerebro se convierta en un reto extraordinario. Aunque\n",
    "todavía estamos lejos de poseer una imagen completa, una imagen parcial siempre\n",
    "es mejor que nada, especialmente si nos da suficiente información como para\n",
    "generar una teoría satisfactoria de la conciencia.\n",
    "El cerebro humano adulto, con poco más de un kilo de peso, contiene unas cien\n",
    "mil millones de células nerviosas o neuronas. La capa ondulada más exterior o\n",
    "corteza cerebral, que es la parte del cerebro de evolución más reciente, contiene\n",
    "alrededor de 30 mil millones de neuronas y un billón de conexiones o sinapsis.\n",
    "Si contáramos una sinapsis cada segundo, tardaríamos 32 millones de a˜nos en\n",
    "acabar el recuento. Si consideramos el número posible de circuitos neuronales,\n",
    "tenemos que habérnoslas con cifras hiperastrónomicas: 10 seguido de al menos un\n",
    "millón de ceros. En comparación, el número de partículas del universo conocido\n",
    "asciende a \"tan solo\" 10 seguido de 79 ceros.\\\n",
    "\\\n",
    "Las siguientes son palabras (con sus similares) clave: **cerebro, ciencia, computación, conciencia, evolución, genética, humano, imagen, medicina, neurona, razón y sinapsis.**\\\n",
    "\\\n",
    "**i.** Escriba el vector de frecuencias v para el anterior texto.\\\n",
    "\\\n",
    "Para obtener el vector de frecuencias observemos el número de veces que se repiten las palabras en el texto, como podemos ver hay 4 palabras que no aparecen **(computación, genética, medicina y razón)**\\\n",
    "\\\n",
    "<img src=\"https://storage.googleapis.com/bucket-adomi-dev/images/Colores.png\">\\\n",
    "\\\n",
    "Siguiendo el orden en que nos dieron las palabras, el vector de frecuencias sería:\\\n",
    "\\\n",
    "$\\mathbf{v}=[5,1,0,1,1,0,2,2,0,3,0,2]$\n",
    "\n",
    "**ii.** Se tienen los siguientes patrones clasificados\\\n",
    "\\\n",
    "$\\begin{aligned}\n",
    "&\\{([5,2,0,0,3,2,1,0,0,3,0,3], A),([3,1,0,1,0,2,0,1,0,4,1,2], A) \\\\\n",
    "&([0,2,3,0,0,0,2,3,2,0,1,0], B),([3,1,4,0,0,0,3,2,3,2,0,4], B), \\\\\n",
    "&([0,3,4,0,0,0,3,2,0,0,2,0], B),([3,2,3,0,0,2,2,0,0,3,0,4], A)\\}\n",
    "\\end{aligned}$\\\n",
    "\\\n",
    "Use el algoritmo de clasificación $k-\\mathrm{NN}$ con $k=3$ para determinar a qué clase pertenecería el vector de frecuencias del texto. Use la métrica del coseno.\\\n",
    "\\\n",
    "\\\n",
    "**Solución**\\\n",
    "\\\n",
    "Para **la metrica del coseno** usamos la ecuación que nos da el ángulo entre dos vectores:\\\n",
    "\\\n",
    "$\\cos \\phi=\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\cdot\\|\\mathbf{v}\\|}$\\\n",
    "\\\n",
    "De donde obtenemos que:\\\n",
    "\\\n",
    "$\\phi= \\arccos (\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\cdot\\|\\mathbf{v}\\|})$\\\n",
    "\\\n",
    "**Apliquemos el algoritmo manualmente**\\\n",
    "\\\n",
    "Vamos a nombrar los vectores del $DTrain$ de la forma $\\mathbf{v}_\\mathbf{1}, \\mathbf{v}_\\mathbf{2},...,\\mathbf{v}_\\mathbf{n}$\\\n",
    "\\\n",
    "Donde $\\mathbf{n}$ es el tamaño del $DTrain$\\\n",
    "\\\n",
    "Tenemos que nuestro vector de frecuencias es:\\\n",
    "\\\n",
    "$\\mathrm{v}= (5\\hspace{0.1cm}1\\hspace{0.1cm}0\\hspace{0.1cm}1\\hspace{0.1cm}1\\hspace{0.1cm}0\\hspace{0.1cm}2\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}3\\hspace{0.1cm}0\\hspace{0.1cm}2)$\\\n",
    "\\\n",
    "Calculamos los ángulos entre nuestro vector $\\mathbf{v}$ y cada uno de los vectores del $Dtrain$\\\n",
    "\\\n",
    "Para $\\mathrm{v}_\\mathbf{1}= (5\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}3\\hspace{0.1cm}2\\hspace{0.1cm}1\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}3\\hspace{0.1cm}0\\hspace{0.1cm}3)$ de la clase **A**\\\n",
    "\\\n",
    "$\\phi _{1}= \\arccos (\\frac{\\mathbf{v} \\cdot \\mathbf{v}_\\mathbf{1}}{\\|\\mathbf{v}\\|\\cdot\\|\\mathbf{v}_\\mathbf{1}\\|})=0.53616$\\\n",
    "\\\n",
    "Para $\\mathrm{v}_\\mathbf{2}= (3\\hspace{0.1cm}1\\hspace{0.1cm}0\\hspace{0.1cm}1\\hspace{0.1cm}0\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}1\\hspace{0.1cm}0\\hspace{0.1cm}4\\hspace{0.1cm}1\\hspace{0.1cm}2)$ de la clase **A**\\\n",
    "\\\n",
    "$\\phi _{2}= \\arccos (\\frac{\\mathbf{v} \\cdot \\mathbf{v}_\\mathbf{2}}{\\|\\mathbf{v}\\|\\cdot\\|\\mathbf{v}_\\mathbf{2}\\|})=0.60589$\\\n",
    "\\\n",
    "Para $\\mathrm{v}_\\mathbf{3}= (0\\hspace{0.1cm}2\\hspace{0.1cm}3\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}2\\hspace{0.1cm}3\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}1\\hspace{0.1cm}0)$ de la clase **B**\\\n",
    "\\\n",
    "$\\phi _{3}= \\arccos (\\frac{\\mathbf{v} \\cdot \\mathbf{v}_\\mathbf{3}}{\\|\\mathbf{v}\\|\\cdot\\|\\mathbf{v}_\\mathbf{3}\\|})=1.25782$\\\n",
    "\\\n",
    "Para $\\mathrm{v}_\\mathbf{4}= (3\\hspace{0.1cm}1\\hspace{0.1cm}4\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}3\\hspace{0.1cm}2\\hspace{0.1cm}3\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}4)$ de la clase **B**\\\n",
    "\\\n",
    "$\\phi _{4}= \\arccos (\\frac{\\mathbf{v} \\cdot \\mathbf{v}_\\mathbf{4}}{\\|\\mathbf{v}\\|\\cdot\\|\\mathbf{v}_\\mathbf{4}\\|})=0.80521$\\\n",
    "\\\n",
    "Para $\\mathrm{v}_\\mathbf{5}= (0\\hspace{0.1cm}3\\hspace{0.1cm}4\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}3\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}2\\hspace{0.1cm}0)$ de la clase **B**\\\n",
    "\\\n",
    "$\\phi _{5}= \\arccos (\\frac{\\mathbf{v} \\cdot \\mathbf{v}_\\mathbf{5}}{\\|\\mathbf{v}\\|\\cdot\\|\\mathbf{v}_\\mathbf{5}\\|})=1.28016$\\\n",
    "\\\n",
    "Para $\\mathrm{v}_\\mathbf{6}= (3\\hspace{0.1cm}2\\hspace{0.1cm}3\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}2\\hspace{0.1cm}2\\hspace{0.1cm}0\\hspace{0.1cm}0\\hspace{0.1cm}3\\hspace{0.1cm}0\\hspace{0.1cm}4)$ de la clase **A**\\\n",
    "\\\n",
    "$\\phi _{6}= \\arccos (\\frac{\\mathbf{v} \\cdot \\mathbf{v}_\\mathbf{6}}{\\|\\mathbf{v}\\|\\cdot\\|\\mathbf{v}_\\mathbf{6}\\|})=0.74956$\\\n",
    "\\\n",
    "Como podemos ver los 3 vecinos más cercanos a nuestro vector $\\mathbf{v}$ son $\\mathbf{v}_\\mathbf{1},\\hspace{0.1cm}\\mathbf{v}_\\mathbf{2}$ y $\\hspace{0.1cm}\\mathbf{v}_\\mathbf{6}$. Todos de la clase **A**, por lo que nuestro vector $\\mathbf{v}$ por mayoría de votos y de forma rotunda se clasifica en la clase **A**\\\n",
    "\\\n",
    "**Ahora corramos el algoritmo en Python**"
   ],
   "attachments": {},
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "MD",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import statistics as st\n",
    "\n",
    "\n",
    "# We define our function of norm of a vector\n",
    "def norm(v):\n",
    "    return np.linalg.norm(np.array(v))\n",
    "\n",
    "\n",
    "# We define our fuction of angle between two vectors\n",
    "def angle(v1, v2):\n",
    "    dot_product = np.dot(np.array(v1), np.array(v2))\n",
    "    division = dot_product / (norm(v1) * norm(v2))\n",
    "    return math.acos(division)\n",
    "\n",
    "\n",
    "# We define our K-NN algorithm\n",
    "def knn(d_train, k, v):\n",
    "    print(\"All neighbors\")\n",
    "    # We calculate the angles between v and all vector from D_Train and sort them ascendingly\n",
    "    angles = []\n",
    "    for data in d_train:\n",
    "        element = [angle(data[0], v), data[0], data[1]]\n",
    "        angles.append(element)\n",
    "        print(element)\n",
    "    angles.sort()\n",
    "    # Here, we have the k nearest neighbors and their votes\n",
    "    nearest_neighbors = angles[:k]\n",
    "    # Here, we have the votes from the k nearest neighbors\n",
    "    votes = []\n",
    "    for n in nearest_neighbors:\n",
    "        votes.append(n[2])\n",
    "    # We print the Nearest neihbors\n",
    "    print('Nearest neighbors')\n",
    "    for neighbor in nearest_neighbors:\n",
    "        print(neighbor)\n",
    "    # We return the most repeated vote\n",
    "    return st.mode(votes)\n",
    "\n",
    "\n",
    "# We define our DTrain, our k and our vector to clasific\n",
    "d_train = [[[5, 2, 0, 0, 3, 2, 1, 0, 0, 3, 0, 3], 'A'], [[3, 1, 0, 1, 0, 2, 0, 1, 0, 4, 1, 2], 'A'],\n",
    "           [[0, 2, 3, 0, 0, 0, 2, 3, 2, 0, 1, 0], 'B'], [[3, 1, 4, 0, 0, 0, 3, 2, 3, 2, 0, 4], 'B'],\n",
    "           [[0, 3, 4, 0, 0, 0, 3, 2, 0, 0, 2, 0], 'B'], [[3, 2, 3, 0, 0, 2, 2, 0, 0, 3, 0, 4], 'A']]\n",
    "k = 3\n",
    "v = [5, 1, 0, 1, 1, 0, 2, 2, 0, 3, 0, 2]\n",
    "\n",
    "# We run our K-NN algorithm\n",
    "group_of_v = knn(d_train, k, v)\n",
    "print(f'The vector {v} belong to class {group_of_v}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "All neighbors\n",
      "[0.5361608636338333, [5, 2, 0, 0, 3, 2, 1, 0, 0, 3, 0, 3], 'A']\n",
      "[0.6058911188392463, [3, 1, 0, 1, 0, 2, 0, 1, 0, 4, 1, 2], 'A']\n",
      "[1.2578167769497866, [0, 2, 3, 0, 0, 0, 2, 3, 2, 0, 1, 0], 'B']\n",
      "[0.8052112713721369, [3, 1, 4, 0, 0, 0, 3, 2, 3, 2, 0, 4], 'B']\n",
      "[1.280158496140076, [0, 3, 4, 0, 0, 0, 3, 2, 0, 0, 2, 0], 'B']\n",
      "[0.7495604359030565, [3, 2, 3, 0, 0, 2, 2, 0, 0, 3, 0, 4], 'A']\n",
      "Nearest neighbors\n",
      "[0.5361608636338333, [5, 2, 0, 0, 3, 2, 1, 0, 0, 3, 0, 3], 'A']\n",
      "[0.6058911188392463, [3, 1, 0, 1, 0, 2, 0, 1, 0, 4, 1, 2], 'A']\n",
      "[0.7495604359030565, [3, 2, 3, 0, 0, 2, 2, 0, 0, 3, 0, 4], 'A']\n",
      "The vector [5, 1, 0, 1, 1, 0, 2, 2, 0, 3, 0, 2] belong to class A\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "CODE",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.** El siguiente algoritmo, llamado WINNOW, es una alternativa para el problema de optimización en problemas de clasificación binaria. Los vectores son binarios (tienen componentes 0 y 1 ) y las clases tienen como etiquetas 0 y 1. El clasificador se define como:\n",
    "$$\n",
    "f_{\\mathrm{w}}(\\mathbf{e})= \\begin{cases}1 & \\text { Si } w \\cdot \\mathbf{e}>\\theta \\\\ 0 & \\text { En otro caso }\\end{cases}\n",
    "$$\n",
    "$\\theta$ es llamado un valor umbral. En este caso no se le agrega 1 a los vectores de entrenamiento.\\\n",
    "\\\n",
    "**Algoritmo WINNOW.**\\\n",
    "\\\n",
    "**i.** Inicialize los pesos en $w_{i}=1$.\\\n",
    "\\\n",
    "**ii.** Haga el umbral $\\theta=n-0.1$, donde $n$ es el número de atributos y haga $\\alpha=2$.\\\n",
    "\\\n",
    "**iii.** Presente un ejemplo de entrenamiento $<\\mathbf{e}, y>$ y calcule $\\hat{y}=f_{\\mathrm{w}}(\\mathbf{e})$\\\n",
    "\\\n",
    "**iv.** Si $y \\neq \\hat{y}$, actualice los pesos de cada atributo cuyo valor sea $x_{i}=1$ :\\\n",
    "\\\n",
    "$\\hspace{5.5cm}w_{i}=w_{i} \\cdot \\alpha^{y-\\hat{y}}$\\\n",
    "\\\n",
    "**v.** Si $y=\\hat{y}$ para todos los ejemplos de entrenamiento, parar; si no, volver al paso iii).\\\n",
    "\\\n",
    "Aplique el anterior algoritmo a los datos de la tabla.\\\n",
    "\\\n",
    "$\\mathbf{e}_{1}= (1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0),\\hspace{0.4cm}{y}_{1}=1$\\\n",
    "\\\n",
    "$\\mathbf{e}_{2}= (0\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 0\\hspace{0.1cm} 0),\\hspace{0.4cm}{y}_{2}=0$\\\n",
    "\\\n",
    "$\\mathbf{e}_{3}= (1\\hspace{0.1cm} 1 \\hspace{0.1cm}0\\hspace{0.1cm} 0 \\hspace{0.1cm}1),\\hspace{0.4cm}{y}_{3}=1$\\\n",
    "\\\n",
    "$\\mathbf{e}_{4}= (0\\hspace{0.1cm} 1\\hspace{0.1cm} 1 \\hspace{0.1cm}1 \\hspace{0.1cm}0),\\hspace{0.4cm}{y}_{4}=0$\\\n",
    "\\\n",
    "$\\mathbf{e}_{5}= (1\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 1\\hspace{0.1cm} 1),\\hspace{0.4cm}{y}_{5}=0$\\\n",
    "\\\n",
    "**Solución**\\\n",
    "\\\n",
    "Vamos a correr la primer iteración del algortimo manualmente\\\n",
    "\\\n",
    "**Pasos**\\\n",
    "\\\n",
    "**i.** $w=(1\\hspace{0.1cm} 1\\hspace{0.1cm}1 \\hspace{0.1cm}1 \\hspace{0.1cm}1)$\\\n",
    "\\\n",
    "**ii.** $n=5$, por lo tanto, $\\theta=4.9$ y $\\alpha=2$\\\n",
    "\\\n",
    "**iii.** Ejemplo de entrenamiento, $\\mathbf{e}_{1}= (1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0)\\hspace{0.4cm}{y}_{1}=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w \\cdot \\mathbf{e}_{1}=(1\\hspace{0.1cm} 1\\hspace{0.1cm}1 \\hspace{0.1cm}1 \\hspace{0.1cm}1) \\cdot (1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0) = 3$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Como $w \\cdot \\mathbf{e}_{1} < \\theta$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Entonces $\\hspace{0.4cm}\\hat{y}=f_{\\mathrm{w}}(\\mathbf{(1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0)})=0$\\\n",
    "\\\n",
    "**iv.** Tenemos $y=1$ y $\\hat{y}=0$, como se cumple la condición de que: $y \\neq \\hat{y}$, actualizamos en el vector $w$ todas las posiciones $w_{i}$ tal que $\\mathbf{e}_{i}=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w=(2\\hspace{0.1cm} 2\\hspace{0.1cm}1 \\hspace{0.1cm}2 \\hspace{0.1cm}1)$\\\n",
    "\\\n",
    "**v.** Calculamos $\\hat{y}$ para todos los ejemplos de entrenamiento\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Para $\\mathbf{e}_{1}=(1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0), \\hspace{0.4cm} y_{1}=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w \\cdot \\mathbf{e}_{1}=(2\\hspace{0.1cm} 2\\hspace{0.1cm}1 \\hspace{0.1cm}2 \\hspace{0.1cm}1) \\cdot (1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0) = 6$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Como $w \\cdot \\mathbf{e}_{1} > \\theta$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Entonces $\\hspace{0.4cm}\\hat{y}=f_{\\mathrm{w}}(\\mathbf{(1\\hspace{0.1cm} 1\\hspace{0.1cm}0 \\hspace{0.1cm}1 \\hspace{0.1cm}0)})=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Se cumple $y=\\hat{y}$\\\n",
    "\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Para $\\mathbf{e}_{2}=(0\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 0\\hspace{0.1cm} 0), \\hspace{0.4cm} y_{2}=0$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w \\cdot \\mathbf{e}_{2}=(2\\hspace{0.1cm} 2\\hspace{0.1cm}1 \\hspace{0.1cm}2 \\hspace{0.1cm}1) \\cdot (0\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 0\\hspace{0.1cm} 0) = 3$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Como $w \\cdot \\mathbf{e}_{2} < \\theta$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Entonces $\\hspace{0.4cm}\\hat{y}=f_{\\mathrm{w}}(\\mathbf{(0\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 0\\hspace{0.1cm} 0)})=0$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Se cumple $y=\\hat{y}$\\\n",
    "\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Para $\\mathbf{e}_{3}=(1\\hspace{0.1cm} 1 \\hspace{0.1cm}0\\hspace{0.1cm} 0 \\hspace{0.1cm}1), \\hspace{0.4cm} y_{3}=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w \\cdot \\mathbf{e}_{3}=(2\\hspace{0.1cm} 2\\hspace{0.1cm}1 \\hspace{0.1cm}2 \\hspace{0.1cm}1) \\cdot (1\\hspace{0.1cm} 1 \\hspace{0.1cm}0\\hspace{0.1cm} 0 \\hspace{0.1cm}1) = 5$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Como $w \\cdot \\mathbf{e}_{3} > \\theta$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Entonces $\\hspace{0.4cm}\\hat{y}=f_{\\mathrm{w}}(\\mathbf{(1\\hspace{0.1cm} 1 \\hspace{0.1cm}0\\hspace{0.1cm} 0 \\hspace{0.1cm}1)})=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Se cumple $y=\\hat{y}$\\\n",
    "\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Para $\\mathbf{e}_{4}=(0\\hspace{0.1cm} 1\\hspace{0.1cm} 1 \\hspace{0.1cm}1 \\hspace{0.1cm}0), \\hspace{0.4cm} y_{4}=0$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w \\cdot \\mathbf{e}_{4}=(2\\hspace{0.1cm} 2\\hspace{0.1cm}1 \\hspace{0.1cm}2 \\hspace{0.1cm}1) \\cdot (0\\hspace{0.1cm} 1\\hspace{0.1cm} 1 \\hspace{0.1cm}1 \\hspace{0.1cm}0) = 5$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Como $w \\cdot \\mathbf{e}_{4} > \\theta$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Entonces $\\hspace{0.4cm}\\hat{y}=f_{\\mathrm{w}}(\\mathbf{(0\\hspace{0.1cm} 1\\hspace{0.1cm} 1 \\hspace{0.1cm}1 \\hspace{0.1cm}0)})=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$No se cumple $y=\\hat{y}$\\\n",
    "\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Para $\\mathbf{e}_{5}=(1\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 1\\hspace{0.1cm} 1), \\hspace{0.4cm} y_{5}=0$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}w \\cdot \\mathbf{e}_{5}=(2\\hspace{0.1cm} 2\\hspace{0.1cm}1 \\hspace{0.1cm}2 \\hspace{0.1cm}1) \\cdot (1\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 1\\hspace{0.1cm} 1) = 8$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Como $w \\cdot \\mathbf{e}_{5} > \\theta$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Entonces $\\hspace{0.4cm}\\hat{y}=f_{\\mathrm{w}}(\\mathbf{(1\\hspace{0.1cm} 1 \\hspace{0.1cm}1\\hspace{0.1cm} 1\\hspace{0.1cm} 1)})=1$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$No se cumple $y=\\hat{y}$\\\n",
    "\\\n",
    "$\\hspace{0.4cm}$Ahora lo que debemos hacer es volver al **paso iii** y repetir el proceso con $\\mathbf{e}_{4}$, luego si es necesario hacer lo mismo con $\\mathbf{e}_{5}$ y así sucesivamente hasta que se cumpla la condición del **paso v**.\\\n",
    "\\\n",
    "**Veamos a continuación el algoritmo completo en Python**"
   ],
   "attachments": {},
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "MD",
     "sheet_delimiter": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# We define our fw(e) function\n",
    "def fw(e, w, threshold):\n",
    "    e = np.array(e)\n",
    "    w = np.array(w)\n",
    "    we = np.dot(w, e)\n",
    "    if we > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# We define our winnow algorithm\n",
    "def winnow(d_train, alpha):\n",
    "    # i. We create the w vector and initialize it with weights in 1\n",
    "    w = []\n",
    "    e1 = d_train[0][0]\n",
    "    for _ in e1:\n",
    "        w.append(1)\n",
    "    # ii. We create the threshold\n",
    "    threshold = len(w) - 0.1\n",
    "    # iii. We start with one training example\n",
    "    answer = False\n",
    "    j = 0\n",
    "    while not answer:\n",
    "        e = d_train[j][0]\n",
    "        y = d_train[j][1]\n",
    "        y_hat = fw(e, w, threshold)\n",
    "        # iv. If y != y_hat, we update the vector w\n",
    "        if y != y_hat:\n",
    "            i = 0\n",
    "            while i < len(w):\n",
    "                if e[i] == 1:\n",
    "                    w[i] *= alpha ** (y - y_hat)\n",
    "                i += 1\n",
    "        print(f'\\nNow, w = {w}')\n",
    "        # v. We calculate y_hat for all training examples\n",
    "        good = True\n",
    "        for data in d_train:\n",
    "            e_ = data[0]\n",
    "            y_ = data[1]\n",
    "            y_hat_ = fw(e_, w, threshold)\n",
    "            if y_ != y_hat_:\n",
    "                good = False\n",
    "            # We print the data\n",
    "            i = d_train.index(data) + 1\n",
    "            print(f'\\nPara e{i} = {e_}, y{i} = {y_}')\n",
    "            print(f'y_hat = {y_hat_}')\n",
    "            print('Se cumple' if good else 'No se cumple', ' y = y_hat')\n",
    "        answer = good\n",
    "        if j == len(d_train) - 1:\n",
    "            j = 0\n",
    "        else:\n",
    "            j += 1\n",
    "    print('\\nAlgoritm finished')\n",
    "\n",
    "\n",
    "# We define our DTrain and our alpha\n",
    "\"\"\" Data from workshop\n",
    "d_train = [[[1, 1, 0, 1, 0], 1], [[0, 1, 1, 0, 0], 0], [[1, 1, 0, 0, 1], 1],\n",
    "           [[0, 1, 1, 1, 0], 0], [[1, 1, 1, 1, 1], 0]]\n",
    "\"\"\"\n",
    "# Data that I invented\n",
    "d_train = [[[1, 1, 0, 1, 0], 1], [[0, 1, 1, 0, 0], 0], [[1, 1, 0, 0, 1], 1],\n",
    "           [[1, 0, 1, 1, 0], 0], [[0, 1, 0, 1, 0], 0]]\n",
    "alpha = 2\n",
    "\n",
    "# We run our winnow algorithm\n",
    "winnow(d_train, alpha)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Now, w = [2, 2, 1, 2, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2, 2, 1, 2, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2, 2, 1, 2, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [1.0, 2, 0.5, 1.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [1.0, 2, 0.5, 1.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 0\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2.0, 4, 0.5, 2.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2.0, 4, 0.5, 2.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2.0, 4, 0.5, 2.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2.0, 4, 0.5, 2.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 1\n",
      "No se cumple  y = y_hat\n",
      "\n",
      "Now, w = [2.0, 2.0, 0.5, 1.0, 1]\n",
      "\n",
      "Para e1 = [1, 1, 0, 1, 0], y1 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e2 = [0, 1, 1, 0, 0], y2 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e3 = [1, 1, 0, 0, 1], y3 = 1\n",
      "y_hat = 1\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e4 = [1, 0, 1, 1, 0], y4 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Para e5 = [0, 1, 0, 1, 0], y5 = 0\n",
      "y_hat = 0\n",
      "Se cumple  y = y_hat\n",
      "\n",
      "Algoritm finished\n"
     ],
     "output_type": "stream"
    }
   ],
   "metadata": {
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "datalore": {
     "type": "CODE",
     "sheet_delimiter": false
    }
   }
  }
 ],
 "metadata": {
  "datalore": {
   "version": 1,
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "base_environment": "default",
   "packages": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}